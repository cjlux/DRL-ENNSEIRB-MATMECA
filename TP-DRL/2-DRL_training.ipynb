{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################################################################\n",
    "# Copyright 2022 Jean-Luc CHARLES\n",
    "# Created: 2022-12-04\n",
    "# version: 1.1 - 7 Dec 2022 \n",
    "# License: GNU GPL-3.0-or-later\n",
    "###########################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <img align=\"left\" src=\"./images/movie_camera.png\" width=\"40\" height=\"40\">_Deep Reinforcement Learning_ (DRL) to train a PPO neural network control the robot position.\n",
    "\n",
    "In this notebook you will learn how to use the DRL to train a ___PPO___  neural network to control the robot position.\n",
    "\n",
    "# <img align=\"left\" src=\"./images/film_reel.png\" width=\"40\" height=\"40\">$\\,$Outline <a name=\"top\"></a>\n",
    "- [ 1 $-$ The RoboticArm_2DOF class](#1)\n",
    "    - [ 1.1 $-$ Instancite the robot](#1.1)\n",
    "    - [ 1.2 $-$ Run a simple test](#1.2)\n",
    "- [ 2 $-$ Train the PPO neural network](#2)\n",
    "    - [ 2.1 $-$ A first _fake_ training to see](#2.1)\n",
    "    - [ 2.2 $-$ Run the full training](#2.2)\n",
    "    - [ 2.3 $-$ Find the best training epoch](#2.3)\n",
    "    - [ 2.4 $-$ Evaluate the trained agent performance](#2.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recommended links:\n",
    "- Pybullet online documentation: [PyBullet Quickstart Guide](https://docs.google.com/document/d/10sXEhzFRSnvFcl3XxNGhnD4N2SedqwdAvK3dsihxVUA/edit#heading=h.2ye70wns7io3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import the required modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, time, shutil, yaml, pathlib, shutil\n",
    "import pybullet as p\n",
    "import pybullet_data\n",
    "import numpy as np\n",
    "from numpy.linalg import norm       # to get the norm of a vector\n",
    "from numpy import pi\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from utils.tools import is_close_to, display_joint_properties, test_training, sample_line, sample_traj4pts\n",
    "from utils.tools import welcome, plot_test, moving_average, get_files_by_date\n",
    "from utils.RoboticArm_2DOF import RoboticArm_2DOF_PyBullet\n",
    "\n",
    "# the PyBullect connection:\n",
    "pc = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. $-$ The RoboticArm_2DOF_PyBullet class <a name=\"1\"></a>\n",
    "\n",
    "The RoboticArm_2DOF class inherits from the base class `env` of the framework _Gym_.<br>\n",
    "It is defined in the file `utils/RoboticArm_2DOF.py` with main tasks : \n",
    "- Create an instance of the Env class.\n",
    "- Create a PyBullet session.\n",
    "- Instanciate the 2 DOF robot arm in the simulator session using its URDF file.\n",
    "- Make the simulated robot move under the actions gven by the PPO agent.. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 $-$ Instanciate the robot <a name=\"1.1\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "URDF   = \"./urdf/RoboticArm_2DOF_2.urdf\"\n",
    "TARGET = \"./urdf/target.urdf\"\n",
    "\n",
    "if 'env' in dir(): \n",
    "    env.close()\n",
    "    del env\n",
    "\n",
    "env = RoboticArm_2DOF_PyBullet(robot_urdf = URDF,       # mandatory\n",
    "                               target_urdf = TARGET,    # mandatory\n",
    "                               dt = 0.002,              # mandatory\n",
    "                               headless = False,        # to get the PyBullet graphical interface \n",
    "                               verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 - Run a simple kinematic test<a name=\"1.2\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this test, the two DOFs of the robot (angles $q_1$ and $q_2$) are controlled by value to make the robot move around its start position."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = env.testAngleControl(0.075)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot the data:\n",
    "\n",
    "We can verify that tha actual values of $q_1$ and $q_2$ are close to the tagrte values (dashed black lines).<br>\n",
    "We also draw the velocities $\\dot{q}_1$ and $\\dot{q}_2$, as well as the trajectory of the end effector of the robot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = data.astype(float)\n",
    "plot_test(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try the reset() method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "obs = env.reset(options={\"target_initial_pos\":(0.50, 0, 0.50), 'randomize':True})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = env.reset(options={\"target_initial_pos\":(0.50, 0, 0.50), 'randomize':True})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = env.reset(options={\"target_initial_pos\":(0.50, 0, 0.50), 'randomize':False})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = env.reset(options={\"target_initial_pos\":(0.50, 0, 0.50), 'randomize':False})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Close the envrironment\n",
    "\n",
    "When you have done with the `env` object, before creating another `RoboticArm_2DOF_PyBullet` it is important to close the current `env` properly in order to close the Pybullet session ans all what is connected with the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[top](#top)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 - Train the PPO neural network <a name=\"2\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training Strategy\n",
    "\n",
    "The training of the robot involves a loop on the target position:\n",
    "- A random target position is choosen in the space reachable by the robot.\n",
    "- During the training, the reward function encourages the robot to move its end effector as close as possible to the target position.\n",
    "- When done (the end effector is close enough to the target), a new target position is randomly choosen, and so on...\n",
    "\n",
    "The whole training process is driven by many __hyperparameters__ including:\n",
    "- `tot_steps`: the total number of random positions learned before we decide that the network is trained.\n",
    "- `EPSILON`: the distance between the end effector and the target mass below which the effector is considered close enough to the target.\n",
    "- `n_epoch`: the (classical) number of training runs with the same data set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 A first _fake_ training to see <a name=\"2.1\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will run now a first _fake_ training to see if all is OK : the purpose of this section is just to show you the steps of a DRL training scenario. <br>\n",
    "At this stage, you will use the __reward function__ `reward_0` already defined in the file `reward.py` : it returns always 0, so the PPO agent wil not learn anything....<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.1 Instanciate the robot <a name=\"2.1.1\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's define important parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DT      = 0.002          # the simulation time step\n",
    "EPSILON = 1e-3           # the distance threshold betwwen the end effector and the target\n",
    "SEED    = 1234567        # the sedd for the random generators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create an instance of `RoboticArm_2DOF_PyBullet`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "URDF   = \"./urdf/RoboticArm_2DOF_2.urdf\"\n",
    "TARGET = \"./urdf/target.urdf\"\n",
    "\n",
    "if 'env' in dir():\n",
    "    try:\n",
    "        env.close()\n",
    "    except:    \n",
    "        del env\n",
    "\n",
    "env    = RoboticArm_2DOF_PyBullet(robot_urdf  = URDF, \n",
    "                                  target_urdf = TARGET, \n",
    "                                  dt = DT,\n",
    "                                  init_robot_angles = (113, -140),\n",
    "                                  init_target_pos = (0.5, 0, 0.5),\n",
    "                                  reward = 'reward_0',\n",
    "                                  seed = SEED,\n",
    "                                  epsilon = EPSILON,\n",
    "                                  headless = False,  # we kep the graphical rendering for this 'fake' round\n",
    "                                  max_episode_steps = 500,\n",
    "                                  verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.2 Instanciate the PPO network <a name=\"2.1.2\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define the PPO training hyperparameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy     = 'MlpPolicy'\n",
    "tot_steps  = 20000       # 'just' 20000 steps for this 'fake' round\n",
    "save_freq  = 5000        # save the networks weights every 'save_freq' steps\n",
    "nb_steps   = 2048        # The number of steps to run per update (the size of the rollout buffer)\n",
    "nb_epochs  = 10          # number of training iterations with the same dataset\n",
    "batch_size = 256         # size of the batch to train the network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define automatically a uniq name for the training directory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_time = time.localtime()\n",
    "experiment_id = \"_\".join(['2DOF_RobotArm_PyBullet', 'PPO', time.strftime(\"%y-%m-%d_%H-%M-%S\", experiment_time)])\n",
    "\n",
    "training_dir = pathlib.Path('models')/experiment_id\n",
    "training_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"Training in directory <{training_dir}>\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the PPO neural network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.callbacks import CheckpointCallback\n",
    "\n",
    "agent = PPO(policy, \n",
    "            env, \n",
    "            n_epochs = nb_epochs,\n",
    "            n_steps = nb_steps,\n",
    "            batch_size = batch_size,\n",
    "            use_sde = False,\n",
    "            seed = SEED,\n",
    "            tensorboard_log = training_dir,\n",
    "            verbose=1)\n",
    "\n",
    "checkpoint_callback = CheckpointCallback(save_freq = save_freq, \n",
    "                                         save_path = training_dir/'ZIP')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now train the network<br>\n",
    "(you can find an explanation of the training display here : https://stable-baselines3.readthedocs.io/en/master/common/logger.html#rollout)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# train agent\n",
    "t0 = time.time()\n",
    "\n",
    "agent.learn(total_timesteps=tot_steps, callback=checkpoint_callback)\n",
    "    \n",
    "t = int(time.time()-t0)\n",
    "h = int(t/3600)\n",
    "m = int((t - h*3600)/60)\n",
    "print(f\"Training elapsed time: {h:02d}h {m:02d}m\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### $\\blacktriangleright$ Check target positions:\n",
    "\n",
    "$\\blacktriangleright$  Using the coordinates `x`and `z` of the attribut `target_pos` of `env` object, try to plot the successive positions of the target during training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### $\\blacktriangleright$  Check robot end effector positions:\n",
    "\n",
    "$\\blacktriangleright$  Using the attribut `ee_pos` of `env`, try to plot the successive positions of the robot end effector during training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[top](#top)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 $-$ Run the full training <a name=\"2.2\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DT      = 0.002          # the simulation time step\n",
    "EPSILON = 1e-3           # the distance threshold betwwen the end effector and the target\n",
    "SEED    = 1234567        # the sedd for the random generators\n",
    "MAX_EPISODE_STEPS = 500"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create an instance of `RoboticArm_2DOF_PyBullet`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "URDF   = \"./urdf/RoboticArm_2DOF_2.urdf\"\n",
    "TARGET = \"./urdf/target.urdf\"\n",
    "\n",
    "if 'env' in dir():\n",
    "    try:\n",
    "        env.close()\n",
    "    except:    \n",
    "        del env\n",
    "\n",
    "env    = RoboticArm_2DOF_PyBullet(robot_urdf  = URDF, \n",
    "                                  target_urdf = TARGET, \n",
    "                                  dt = DT,\n",
    "                                  init_robot_angles = (113, -140),\n",
    "                                  reward = 'reward_1',\n",
    "                                  seed = SEED,\n",
    "                                  epsilon = EPSILON,\n",
    "                                  headless = True,  # no more graphical rendering for this round\n",
    "                                  max_episode_steps = MAX_EPISODE_STEPS,\n",
    "                                  verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PPO hyperparameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = 'MlpPolicy'\n",
    "tot_steps  = 5000000     # will take a few hours...\n",
    "save_freq  = 100000      # save the networks weights every 'save_freq' steps\n",
    "nb_steps   = 2048        # The number of steps to run per update (the size of the rollout buffer)\n",
    "nb_epochs  = 10          # number of training iterations with the same dataset\n",
    "batch_size = 256         # size of the batch to train the network\n",
    "headless   = True        # no graphical renering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define automatically a uniq name for the training directory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_time = time.localtime()\n",
    "experiment_id = \"_\".join(['2DOF_RobotArm_PyBullet', 'PPO', time.strftime(\"%y-%m-%d_%H-%M-%S\", experiment_time)])\n",
    "\n",
    "training_dir = pathlib.Path('models')/experiment_id\n",
    "training_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"Training in directory <{training_dir}>\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now train the network<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.callbacks import CheckpointCallback\n",
    "\n",
    "agent = PPO(policy, \n",
    "            env, \n",
    "            n_epochs = nb_epochs,\n",
    "            n_steps = nb_steps,\n",
    "            batch_size = batch_size,\n",
    "            use_sde = False,\n",
    "            seed = SEED,\n",
    "            tensorboard_log = training_dir,\n",
    "            verbose = 1)\n",
    "\n",
    "checkpoint_callback = CheckpointCallback(save_freq = save_freq, \n",
    "                                         save_path = training_dir/'ZIP')\n",
    "\n",
    "# train agent\n",
    "t0 = time.time()\n",
    "\n",
    "agent.learn(total_timesteps = tot_steps, \n",
    "            callback = checkpoint_callback)\n",
    "    \n",
    "t = int(time.time()-t0)\n",
    "h = int(t//3600)\n",
    "m = int((t - h*3600)//60)\n",
    "print(f\"Training elapsed time: {h:02d}h {m:02d}m\")\n",
    "  \n",
    "# save trained agent\n",
    "target_zip = os.path.join(training_dir/'ZIP'/'model.zip')\n",
    "print(f\"saving trained model in <{target_zip}>\")\n",
    "agent.save(target_zip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[top](#top)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 $-$ Find the best training epoch <A name=\"2.3\"> </A>\n",
    "\n",
    "The goal here is to find which saved weights give the best training ?<br><br>\n",
    "\n",
    "We will browse the zip files of the saved weights that are in the `training_dir`: we reload the agent with the saved weights and we let the the agent control the robot to reache five successive target positions defining a diamond.\n",
    "\n",
    "We compute the mean distance betwen the robot end effector and the five target positions : the best epoch is the one where the mean error is minimal.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_dir = pathlib.Path(\"models/..... name of training direcory ..../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DT      = 0.002          # the simulation time step\n",
    "EPSILON = 1e-3           # the distance threshold betwwen the end effector and the target\n",
    "SEED    = 1234567        # the sedd for the random generators\n",
    "MAX_EPISODE_STEPS = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if 'env' in  dir():\n",
    "    try:\n",
    "        env.close()\n",
    "    except:    \n",
    "        del env\n",
    "      \n",
    "URDF   = \"./urdf/RoboticArm_2DOF_2.urdf\"\n",
    "TARGET = \"./urdf/target.urdf\"\n",
    "\n",
    "env    = RoboticArm_2DOF_PyBullet(robot_urdf  = URDF, \n",
    "                                  target_urdf = TARGET, \n",
    "                                  dt = DT,\n",
    "                                  init_robot_angles = (113, -140),\n",
    "                                  reward = 'reward_1',\n",
    "                                  seed = SEED,\n",
    "                                  headless = True, \n",
    "                                  max_episode_steps = None,\n",
    "                                  verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the list of the saved weights files :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_files = get_files_by_date(training_dir/'ZIP')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let the agent control the robot displacement to reach the target:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from stable_baselines3 import PPO\n",
    "\n",
    "err_mean  = 1e20\n",
    "error     = []\n",
    "max_steps = 500\n",
    "\n",
    "for i, file in enumerate(list_files):    \n",
    "    print(file, end=\"\")\n",
    "    err = []\n",
    "    \n",
    "    agent = PPO.load(training_dir/'ZIP'/file)\n",
    "    \n",
    "    obs   = env.reset(options={\"dt\": DT, \n",
    "                               \"target_initial_pos\": (0.5,0,0),\n",
    "                               \"robot_initial_angle_deg\": (113, -140),\n",
    "                               \"randomize\": False,\n",
    "                               \"epsilon\": EPSILON})    \n",
    "    \n",
    "    for target_pos in ((0.5,0.,0.02), (1,0,0.5), (0.5,0,1), (0,0,0.5), (0.5,0,0.02)):\n",
    "        env.set_target_position(np.array(target_pos))\n",
    "        done, step_count, rewards, actions = False, 0, [], []\n",
    "        while step_count < max_steps:\n",
    "            action, _ = agent.predict(obs, deterministic=True)\n",
    "            obs, reward, done, info = env.step(action)\n",
    "            step_count += 1\n",
    "            if done: break        \n",
    "        dist_effect_target = norm(np.array(env.effector_pos) - target_pos)\n",
    "        err.append(dist_effect_target)\n",
    "        \n",
    "    e_mean = np.array(err).mean()\n",
    "    e_std  = np.array(err).std()\n",
    "    error.append(err)\n",
    "    print(f\"\\t e_mean: {e_mean*100:6.2f}, e_std: {e_std*100:6.2f} cm\")\n",
    "    if e_mean < err_mean: best_train, err_mean = file, e_mean\n",
    "            \n",
    "error = np.array(error)            \n",
    "print(f\"Best train: {best_train:30s}, error: {err_mean*100:.2f} cm\")    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute for file the mean of position errors for the 5 target :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error.mean(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\blacktriangleright$  Plot the mean errors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\blacktriangleright$  Find the rank of the file correspondint to the smallest error:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rank_min_error = 31"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\blacktriangleright$  Print the name of the zip file corresponding to the smallest mean error, and the value of the smallest mean error in __cm__:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[top](#top)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 $-$ Evaluate the trained agent performance <A name=\"2.4\"></A>\n",
    "\n",
    "Now we will display the behaviour of the robot controlled by the best trained PPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = list_files[rank_min_error]\n",
    "print(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = PPO.load(training_dir/'ZIP'/file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "URDF   = \"./urdf/RoboticArm_2DOF_2.urdf\"\n",
    "TARGET = \"./urdf/target.urdf\"\n",
    "\n",
    "if 'env' in  dir():\n",
    "    try:\n",
    "        env.close()\n",
    "    except:    \n",
    "        del env\n",
    "    \n",
    "env = RoboticArm_2DOF_PyBullet(robot_urdf  = URDF, \n",
    "                               target_urdf = TARGET, \n",
    "                               dt = DT,\n",
    "                               init_robot_angles = (113, -140),\n",
    "                               reward = 'reward_1',\n",
    "                               seed = SEED,\n",
    "                               headless = False, \n",
    "                               max_episode_steps = None,\n",
    "                               verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from stable_baselines3 import PPO\n",
    "\n",
    "results, err_mean = {}, float(\"+inf\")\n",
    "\n",
    "nb_pts_per_line = 100\n",
    "max_step_nb = 50\n",
    "\n",
    "# the four points defing the diamond:\n",
    "p1, p2, p3, p4 = (0.5, 0.02), (1, 0.5), (0.5, 1), (0, 0.5)\n",
    "\n",
    "# the sequence of points defing a closed trajectory:\n",
    "trajectory = ((p2, p3), (p3, p4), (p4, p1), (p1, p2))\n",
    "\n",
    "# sample the 4 points rajectory to get a list of equaly separated target points along the trajectory:\n",
    "pts, dl = sample_traj4pts(trajectory, nb_pts_per_line)\n",
    "\n",
    "# Now reload the agent wit the best weights file:\n",
    "agent = PPO.load(os.path.join(training_dir/'ZIP'/file))\n",
    "\n",
    "# let the agent move the robot to follow successively all the points of the trajectory:\n",
    "err = test_training(agent, env, DT, pts, nSubSteps=max_step_nb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# save important files and the notebook in the current training directory !!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for file in ('./2-DRL_training.ipynb', 'rewards.py', './utils/RoboticArm_2DOF.py', './urdf/RoboticArm_2DOF_2.urdf'):\n",
    "    base = os.path.basename(file)\n",
    "    shutil.copyfile(file, os.path.join(training_dir, base))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "toc-showmarkdowntxt": false,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
